<!DOCTYPE html>
<html lang="en">

<head>

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="">
    <meta name="author" content="Johnny Wang, Liang-Chi Tseng, YC Lee">

    <title>GPU-Accelerated MLPClassifier and MLPRegressor</title>

    <!-- Bootstrap Core CSS -->
    <link href="vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">

    <!-- Custom Fonts -->
    <link href="vendor/font-awesome/css/font-awesome.min.css" rel="stylesheet" type="text/css">
    <link href='https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800' rel='stylesheet' type='text/css'>
    <link href='https://fonts.googleapis.com/css?family=Merriweather:400,300,300italic,400italic,700,700italic,900,900italic' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" href="vendor/highlight/styles/solarized-light.css">
    <link rel="stylesheet" href="css/hl.css">


    <!-- Plugin CSS -->
    <link href="vendor/magnific-popup/magnific-popup.css" rel="stylesheet">

    <!-- Theme CSS -->
    <link href="css/creative.css" rel="stylesheet">

    <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->

</head>

<body id="page-top">

    <nav id="mainNav" class="navbar navbar-default navbar-fixed-top">
        <div class="container-fluid">
            <!-- Brand and toggle get grouped for better mobile display -->
            <div class="navbar-header">
                <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1">
                    <span class="sr-only">Toggle navigation</span> Menu <i class="fa fa-bars"></i>
                </button>
                <a class="navbar-brand page-scroll" href="#page-top">GPGPU Final Project</a>
            </div>

            <!-- Collect the nav links, forms, and other content for toggling -->
            <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
                <ul class="nav navbar-nav navbar-right">
                    <li>
                        <a class="page-scroll" href="#about">Techniques</a>
                    </li>
                    <li>
                        <a class="page-scroll" href="#services">Results and Settings</a>
                    </li>
                    <li>
                        <a class="page-scroll" href="#conclusion">Conclusion</a>
                    </li>
                    <li>
                        <a class="page-scroll" href="#contact">Authors</a>
                    </li>
                </ul>
            </div>
            <!-- /.navbar-collapse -->
        </div>
        <!-- /.container-fluid -->
    </nav>

    <header>
        <div class="header-content">
            <div class="header-content-inner">
              <h1 id="homeHeading">GPU-Accelerated<br>MLP Classifier and Regressor</h1>
                <hr>
                <p class="intro">In the scikit-learn framework, we build GPU-accelerated MLPClassifier and MLPRegressor to have better performance and less computation time.</p>
                <a href="#about" class="btn btn-primary btn-xl page-scroll">Find Out More</a>
                <a href="https://github.com/johnnykwwang/GPGPU_Final_Python_CUDA" class="btn btn-primary btn-xl page-scroll">View on GitHub</a>
            </div>
        </div>
    </header>

    <section class="bg-primary" id="about">
        <div class="container">
            <div class="row">
                <div class="col-lg-8 col-lg-offset-2 text-center">
                    <h2 class="section-heading section-heading-white">Techniques</h2>
                    <hr class="light">
                    
                    <p class="text-faded">GPU is well-known for its powerful parallel computation power, so it is reasonable to offload those large matrix operations to GPU. By carefully inspection of the source code of MLP classifier/regressor, we picked several subroutine and transformed it into CUDA kernels.</p>
                </div>
            </div>
            <div class="row">
                <div class="panel-group" id="accordion" role="tablist" aria-multiselectable="true">
                  <div class="panel panel-default">
                    <div class="panel-heading" role="tab" id="headingOne">
                      <h4 class="panel-title">
                        <a role="button" data-toggle="collapse" data-parent="#accordion" href="#collapseOne" aria-expanded="true" aria-controls="collapseOne">Offload computation intensive tasks to GPU</a>
                      </h4>
                    </div>
                    <div id="collapseOne" class="panel-collapse collapse in" role="tabpanel" aria-labelledby="headingOne">
                      <div class="panel-body">
                        <div class="col-lg-12">
                          The following code snippet computes the gradient of loss with respect to coefs and intercept for specified layer. The original sklearn function computes dot product in CPU and applies some scalar operation to input data. We can transform those operations into an element-wise kernel. This significantly improves the performance of the entire program.
                        </div>
                        <div class="col-lg-6">
                          <h4 class="text-center">CPU Version</h4>
                          <pre><code>
def _compute_loss_grad(self, layer, n_samples, activations, deltas, coef_grads, intercept_grads):

    coef_grads[layer] = safe_sparse_dot(activations[layer].T,
                                        deltas[layer])
    coef_grads[layer] += (self.alpha * self.coefs_[layer])
    coef_grads[layer] /= n_samples

    intercept_grads[layer] = np.mean(deltas[layer], 0)

    return coef_grads, intercept_grads
                        </code></pre></div>
                        <div class="col-lg-6">
                          <h4 class="text-center">GPU Version</h4>
                          <pre><code>
kernel_compute_loss_grad_cuda = cp.ElementwiseKernel(
    'float64 coefs, float64 alpha, float64 n_samples',
    'float64 coef_grads',
    'coef_grads = (coef_grads + alpha * coefs)/ n_samples',
    'kernel_compute_loss_grad_cuda')

def _compute_loss_grad_cuda(self, layer, n_samples, activations, deltas, coef_grads, intercept_grads):

    cp.dot(activations[layer].T, deltas[layer], out=coef_grads[layer])
    self.kernel_compute_loss_grad_cuda(self.cuda_coefs_[layer], self.alpha, n_samples,coef_grads[layer])
    cp.mean(deltas[layer], 0, out=intercept_grads[layer])

    return coef_grads, intercept_grads
                        </code></pre></div>
                      </div>
                    </div>
                  </div>
                  <div class="panel panel-default">
                    <div class="panel-heading" role="tab" id="headingTwo">
                      <h4 class="panel-title">
                        <a class="collapsed" role="button" data-toggle="collapse" data-parent="#accordion" href="#collapseTwo" aria-expanded="true" aria-controls="collapseTwo">
                          In-place operations
                        </a>
                      </h4>
                    </div>
                    <div id="collapseTwo" class="panel-collapse collapse" role="tabpanel" aria-labelledby="headingTwo">
                      <div class="panel-body">
                        <div class="col-lg-12">
                          Memory operations are expensive on GPU. In Python code you usually do not care about the memory allocation because the Python GC (garbage collection) manages them well. However, memory management becomes a significant issue when you porting the program from CPU to GPU.
                          <br>
                          The original sklearn generates lots of intermediate array during the computation of logistic loss function. For CPU version it seems to be okay because we spend lot of time on computation. However, the GPU computes these so fast that we cannot simply ignore the memory allocation cause by those operations. Instead, we transform these operations into an in-place computation without generating any intermediate buffers.
                        </div>
                        <div class="col-lg-6">
                          <h4 class="text-center">CPU Version</h4>
                          <pre><code>
def log_loss(y_true, y_prob):
    y_prob = np.clip(y_prob, 1e-10, 1 - 1e-10)
    return -np.sum(y_true * np.log(y_prob)) / y_prob.shape[0]
                        </code></pre>
                        </div>
                        <div class="col-lg-6">
                          <h4 class="text-center">GPU Version</h4>
                          <pre><code>
kernel_log_loss = cp.ReductionKernel(
    'float64 y_true, float64 y_prob, float64 size',
    'float64 val',
    'y_true * log( min(max(y_prob,1e-10),1 - 1e-10) ) * -1.0',
    'a + b',
    'val = a / size',
    '0',
    'kernel_log_loss'
)

def log_loss_cuda(y_true, y_prob):
    return kernel_log_loss(y_true, y_prob, y_prob.shape[0])
                        </code></pre></div>
                    </div>
                  </div>
                  </div>
                  <div class="panel panel-default">
                    <div class="panel-heading" role="tab" id="headingThree">
                      <h4 class="panel-title">
                        <a class="collapsed" role="button" data-toggle="collapse" data-parent="#accordion" href="#collapseThree" aria-expanded="false" aria-controls="collapseThree">Custom kernel</a>
                      </h4>
                    </div>
                    <div id="collapseThree" class="panel-collapse collapse" role="tabpanel" aria-labelledby="headingThree">
                      <div class="panel-body">
                        <div class="col-lg-12">Since sklearn is based on Numpy, it is reasonable to accelerate the performance by simply replace the Numpy routines by those in Cupy which optimized the computation by CUDA. However, not every function can be replaced by Cupy version. The following example computes the L2 regularization term. The original sklearn computes the value by a very Python-style code. However, we cannot find suitable candidate in Cupy. Instead of simply replacing Numpy with Cupy which introduces significant overhead, we write a custom kernel to handle the task.</div>
                        <div class="col-lg-6">
                          <h4 class="text-center">CPU Version</h4>
                          <pre><code>
# Add L2 regularization term to loss
values = np.sum(
    np.array([np.dot(s.ravel(), s.ravel()) for s in self.coefs_]))
                        </code></pre>
                        </div>
                        <div class="col-lg-6">
                          <h4 class="text-center">GPU Version</h4>
                          <pre><code>
kernel_l2_regularization = cp.ReductionKernel(
    'T x',
    'T y',
    'x*x',
    'a + b',
    'y = a',
    '0',
    'kernel_l2_regularization'
)

for s in self.cuda_coefs_:
    values += kernel_l2_regularization(s.ravel())
                        </code></pre></div>
                      </div>
                    </div>
                  </div>
                </div>
        </div>
    </section>

    <section id="services">
        <div class="container">
            <div class="row">
                <div class="col-lg-12 text-center">
                    <h2 class="section-heading">Test Results</h2>
                    <hr class="primary">
                </div>
            </div>
        </div>
        <div class="container">
            <div class="row">
              <div class="col-lg-8">
                <h3><strong>MLPRegressor Acceleration - # of Neurons</strong></h3>
                <iframe src="chart_regressor.html" height="400" style="width: 100%; border: none;"></iframe>
              </div>
              <div class="col-lg-4">
                <strong>Controlled Variables</strong><br>
                - iteration：50<br>
                - batch size：Dataset.shape[0]<br>
                <strong>Independent Variables</strong><br>
                - Number of neurons in hidden layer<br>
                <strong>Summary</strong><br>
                <p>In the usage of MLP algorithms, often times there are lots of features in the input dataset.  To get better model, we usually increase the number of neurons.  This makes the number calculation in MLP algorithms increase largely ( for example in the propagation part).  We can compute these calculations in a parallel manner, so we use GPU to do for example the propagation.<br>
                In the chart we can see that when number of neurons increase, the benefit of GPU-acceleration rises.</p>
              </div>
            </div>
            <hr>
            <div class="row">
              <div class="col-lg-8">
                <h3><strong>MLPRegressor Acceleration - Batch Size</strong></h3>
                <iframe src="chart_regressor_by_batch.html" height="400" style="width: 100%; border: none;"></iframe>
              </div>
              <div class="col-lg-4">
                <strong>Controlled Variables</strong><br>
                - iteration：50<br>
                - Number of neurons in hidden layer: (300,)<br>
                <strong>Independent Variables</strong><br>
                - Batch Size<br>
                <strong>Summary</strong><br>
                <p>Batch size is another factor that affects performance and score of a model.  Since sklearn default uses stochastic algorithm to learn, training data will be cut into batches of small data.  Each batch training is dependent, we need to wait for one another batch to complete.  But at 1 batch the training process is parallel-able.  So if we increase the size of batch, GPU-acceleration's benefit rises.  But the score may also decrease.  Thus finding an optimal batch size is very important. </p>
              </div>
            </div>
            <hr>
            <div class="row">
              <div class="col-lg-8 ">
                <h3><strong>MLPClassifier Acceleration - # of Neurons</strong></h3>
                <iframe src="chart_classifier.html" height="400" style="width: 100%; border: none;"></iframe>
              </div>
              <div class="col-lg-4">
                <strong>Controlled Variables</strong><br>
                - iteration：50<br>
                - Batch Size: 200<br>
                <strong>Independent Variables</strong><br>
                - Number of neurons in hidden layer<br>
                <strong>Summary</strong><br>
                <p>We also perform similar GPU-acceleration on MLPClassifier.  GPU's acceleration ratio rises when the number of neuron in hidden layer rise.  It's worth noting that if the number of neurons are too low, GPU's own overhead of transferring data and instructions might ruin the benefit of acceleration, thus lower than CPU version.</p>
              </div>
            </div>

            <hr>
            <div class="row">
              <div class="col-lg-8">
                <h3><strong>MLPClassifier Acceleration - Batch Size</strong></h3>
                <iframe src="chart_classifier_by_batch.html" height="400" style="width: 100%; border: none;"></iframe>
              </div>
              <div class="col-lg-4">
                <strong>Controlled Variables</strong><br>
                - iteration：50<br>
                - Number of neurons in hidden layer: (300,)<br>
                <strong>Independent Variables</strong><br>
                - Batch Size<br>
                <strong>Summary</strong><br>
                <p>We also perform batch size experiement, in this dataset and settings the batch size does not largely affect the speed up.</p>
              </div>
            </div>
        </div>
    </section>


    <aside class="bg-dark">
        <div class="container text-center">
            <div class="call-to-action">
                <h2>Testing Environment and Scenario</h2>
            </div>
            <div class='row'>
              <div class="col-lg-2 col-lg-offset-2">
                Machine:<br>
                <strong>AWS g2.2xLarge</strong>
              </div>
              <div class="col-lg-2">
                CPU:<br>
                <strong>Intel Xeon E5-2670 (Sandy Bridge)</strong>
              </div>
              <div class="col-lg-2">
                GPU:<br>
                <strong>GRID K340<br>( 1,536 CUDA cores / 4GB video memory ) </strong>
              </div>
              <div class="col-lg-2">
                Dataset:<br>
                <strong>MNIST HandWriting Data && Boston Housing Dataset</strong>
              </div>
            </div>
            <hr>
            <div class='row'>
              <div class="col-lg-6 col-lg-offset-3">
                We hope this setting is close to a normal PC, in fact this setting is very old ( around 2013~ ), and this setting is close to a normal PC with i5 processor and a GTX960 graphics card.
              </div>
        </div>
    </aside>

    <section id="conclusion" class='bg-primary' >
        <div class="container">
            <div class="row">
                <div class="col-lg-8 col-lg-offset-2 text-center">
                    <h2 class="section-heading section-heading-white">Conclusion</h2>
                    <hr class="light">
                    <p class="text-faded">
                      We completed a variety settings of experiments, and discovered that with a dataset larger than some size, GPU is capable of outperforming CPU when executing both MLP algorithms, even when it's a normal performance GPU. <br>
                      Our GPU version have up-to <strong>5 times</strong> faster in MLPRegressor, and up-to <strong>33%</strong> speedup in MLPClassifier.
                    </p>
                </div>
            </div>
        </div>
    </section>
    <section id="contact">
        <div class="container">
            <div class="row">
                <div class="col-lg-8 col-lg-offset-2 text-center">
                    <h2 class="section-heading">Authors</h2>
                    <hr class="primary">
                </div>
            </div>
            <div class="row">
                <div class="col-lg-2 col-lg-offset-3 text-center">
                    <h3>Kuan-Wen Wang 王冠文</h3>
                </div>
                <div class="col-lg-2  text-center">
                    <h3>Henry Tseng 曾亮齊</h3>
                </div>
                <div class="col-lg-2  text-center">
                    <h3>YC Lee</h3>
                </div>
            </div>
        </div>
    </section>

    <!-- jQuery -->
    <script src="vendor/jquery/jquery.min.js"></script>

    <!-- Bootstrap Core JavaScript -->
    <script src="vendor/bootstrap/js/bootstrap.min.js"></script>

    <!-- Plugin JavaScript -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery-easing/1.3/jquery.easing.min.js"></script>
    <script src="vendor/scrollreveal/scrollreveal.min.js"></script>
    <script src="vendor/magnific-popup/jquery.magnific-popup.min.js"></script>

    <!-- Theme JavaScript -->
    <script src="js/creative.min.js"></script>

    <script src="vendor/highlight/highlight.pack.js"></script>
    <script>hljs.initHighlightingOnLoad();</script>


</body>

</html>
